{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ba44d9-e719-4e50-a78c-942931654863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, pathlib, zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "HERE = pathlib.Path().resolve()\n",
    "REPO = HERE  # assuming this notebook lives in the repo root\n",
    "sys.path.insert(0, str(REPO))\n",
    "\n",
    "DATA    = REPO / \"data\"\n",
    "OUTPUT  = REPO / \"outputs\"\n",
    "\n",
    "for p in (OUTPUT):\n",
    "    p.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# 3) find & unzip every `train_*.zip` exactly once\n",
    "zip_files = sorted(DATA.glob(\"train_*.zip\"))\n",
    "for zip_path in zip_files:\n",
    "    extract_to = DATA / zip_path.stem \n",
    "    if not extract_to.exists():\n",
    "        print(f\">>> Extracting {zip_path.name} → {extract_to}/\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "            zf.extractall(extract_to)\n",
    "    else:\n",
    "        print(f\"Already extracted: {extract_to}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0141ac-7509-46e8-b2d3-4b330bc2f028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "HERE = Path().resolve() \n",
    "REPO = HERE                      \n",
    "sys.path.insert(0, str(REPO))\n",
    "\n",
    "DATA       = REPO / \"data\"\n",
    "OUTPUT     = REPO / \"outputs\" / \"VGG16_subset_contrast_100\"\n",
    "IMG_SUBSET = DATA / \"train_contrast_100_subset\"  # change to folder names of unzipped files\n",
    "\n",
    "OUTPUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "import numpy as np\n",
    "\n",
    "from functions.network_data2 import NetworkData\n",
    "from functions.image           import ImageDataset\n",
    "import interface_DeepFramework.DeepFramework as DeepF\n",
    "\n",
    "def preproces_imagenet_img(imgs_hr):\n",
    "    img = np.array(imgs_hr)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "           mean=[0.485,0.456,0.406],\n",
    "           std =[0.229,0.224,0.225]\n",
    "        )\n",
    "    ])\n",
    "    return [transform(img)]\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model  = vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n",
    "    model.to(device).eval()\n",
    "    for name, module in model.named_modules():\n",
    "        module.auto_name = name\n",
    "\n",
    "    deepmodel = DeepF.deep_model(model)\n",
    "\n",
    "    layers_interest = [\n",
    "      [\"features.0\", 0],  # conv1_1\n",
    "      [\"features.2\", 0],  # conv1_2\n",
    "      [\"features.5\", 0],  # conv2_1\n",
    "      [\"features.7\", 0],  # conv2_2\n",
    "    ]\n",
    "\n",
    "    Path_images = str(IMG_SUBSET)\n",
    "\n",
    "    dataset = ImageDataset(\n",
    "      src_dataset           = Path_images,\n",
    "      target_size           = (224,224),\n",
    "      preprocessing_function= preproces_imagenet_img,\n",
    "      color_mode            = \"rgb\"\n",
    "    )\n",
    "\n",
    "    Nefesimodel = NetworkData(\n",
    "      model            = deepmodel,\n",
    "      layer_data       = layers_interest,\n",
    "      save_path        = str(OUTPUT),\n",
    "      dataset          = dataset,\n",
    "      default_file_name= \"VGG16_subset_contrast_100\",\n",
    "      input_shape      = [(1,3,224,224)]\n",
    "    )\n",
    "\n",
    "    Nefesimodel.generate_neuron_data()\n",
    "\n",
    "    # 7) activation calculus\n",
    "    Nefesimodel.eval_network()\n",
    "    print(\"Activation Calculus done!\")\n",
    "    Nefesimodel.save_to_disk(\"activations\")\n",
    "\n",
    "    # 8) neuron features\n",
    "    Nefesimodel.calculateNF()\n",
    "    print(\"NF done!\")\n",
    "    Nefesimodel.save_to_disk(\"withNF\")\n",
    "\n",
    "    # 9) color‐selectivity index\n",
    "    for layer in Nefesimodel.get_layers_name():\n",
    "        layer_data = Nefesimodel.get_layer_by_name(layer)\n",
    "        print(\"Computing CSI on\", layer)\n",
    "        for idx in range(Nefesimodel.get_len_neurons_of_layer(layer)):\n",
    "            neuron = Nefesimodel.get_neuron_of_layer(layer, idx)\n",
    "            neuron.color_selectivity_idx_new(\n",
    "              Nefesimodel, layer_data, dataset\n",
    "            )\n",
    "    Nefesimodel.save_to_disk(\"Normal_class\")\n",
    "    print(\"Saved object with CSI included\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8023df-1222-4f09-9f88-f69fef015c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── export_nfs_and_metrics.py ────────────────────────────────────────────────\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Make sure we can import your nefesi code in functions/\n",
    "HERE = Path().resolve()          # if this file/notebook sits at the repo root\n",
    "REPO = HERE                      # else HERE.parent if it's in a subfolder\n",
    "sys.path.insert(0, str(REPO))\n",
    "\n",
    "# 2) define all the relative paths\n",
    "DATA      = REPO / \"data\"                      # where your train_*_subset/ folders live\n",
    "OUTPUT    = REPO / \"outputs\" / \"VGG16_subset_contrast_100\"\n",
    "OBJ_FINAL = OUTPUT / \"Normal_class.obj\"\n",
    "NF_DIR    = OUTPUT / \"nf_grids\"\n",
    "CSV_OUT   = OUTPUT / \"metrics_all.csv\"\n",
    "\n",
    "# 3) layers of interest\n",
    "LAYERS = [\"features.0\", \"features.2\", \"features.5\", \"features.7\"]\n",
    "\n",
    "# 4) ensure output dirs exist\n",
    "NF_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 5) import nefesi machinery\n",
    "from functions.network_data2 import NetworkData\n",
    "\n",
    "def normalize_patch(patch):\n",
    "    \"\"\"Scale an RGB patch to [0,1].\"\"\"\n",
    "    mn, mx = patch.min(), patch.max()\n",
    "    return (patch - mn) / (mx - mn) if mx > mn else patch - mn\n",
    "\n",
    "def main():\n",
    "    # load the fully-populated NetworkData object\n",
    "    nd = NetworkData.load_from_disk(str(OBJ_FINAL))\n",
    "\n",
    "    records = []\n",
    "\n",
    "    # loop through each layer\n",
    "    for layer in LAYERS:\n",
    "        n_neurons = nd.get_len_neurons_of_layer(layer)\n",
    "        cols      = int(math.ceil(math.sqrt(n_neurons)))\n",
    "        rows      = int(math.ceil(n_neurons / cols))\n",
    "\n",
    "        # 1) Make the NF grid\n",
    "        fig, axes = plt.subplots(rows, cols,\n",
    "                                 figsize=(cols*1.5, rows*1.5),\n",
    "                                 squeeze=False)\n",
    "        for idx in range(rows*cols):\n",
    "            r, c = divmod(idx, cols)\n",
    "            ax   = axes[r][c]\n",
    "            ax.axis(\"off\")\n",
    "            if idx < n_neurons:\n",
    "                neuron = nd.get_neuron_of_layer(layer, idx)\n",
    "                patch  = neuron._neuron_feature\n",
    "                ax.imshow(normalize_patch(patch))\n",
    "                ax.set_title(str(idx), fontsize=6)\n",
    "\n",
    "        fig.suptitle(f\"{layer} — all {n_neurons} neurons\", fontsize=12)\n",
    "        plt.tight_layout(rect=[0,0,1,0.95])\n",
    "        out_png = NF_DIR / f\"{layer.replace('.','_')}_all_nfs.png\"\n",
    "        fig.savefig(str(out_png), dpi=150)\n",
    "        plt.close(fig)\n",
    "        print(\"Saved full NF grid →\", out_png)\n",
    "\n",
    "        # 2) collect metrics for CSV\n",
    "        for idx in range(n_neurons):\n",
    "            neuron  = nd.get_neuron_of_layer(layer, idx)\n",
    "            max_act = float(neuron.activations[0])\n",
    "            # call the CSI method to get the value\n",
    "            csi_val = neuron.color_selectivity_idx_new(nd,\n",
    "                                                       nd.get_layer_by_name(layer),\n",
    "                                                       nd.dataset)\n",
    "            records.append({\n",
    "                \"layer\":         layer,\n",
    "                \"neuron_idx\":    idx,\n",
    "                \"max_activation\": max_act,\n",
    "                \"csi\":           csi_val\n",
    "            })\n",
    "\n",
    "    # 3) write out the combined CSV\n",
    "    df = pd.DataFrame(records)\n",
    "    df.to_csv(str(CSV_OUT), index=False)\n",
    "    print(f\"Wrote metrics CSV → {CSV_OUT} ({len(df)} rows)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
